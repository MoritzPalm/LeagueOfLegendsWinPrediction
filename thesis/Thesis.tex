\documentclass[12pt, a4paper, headinclude, twoside, plainheadsepline, open=right, numbers=noenddot, hidelinks, toc=listof, toc=bibliography]{scrreprt}

%\usepackage{showframe}


% WICHTIG: Hier wird nicht BibTeX sondern BibLateX verwendet!!
% Deshalb nicht mit bibtex uebersetzen, sondern mit biber
% Das kann man in jedem Tool wie TexMaker oder TexShop als Option einstellen
%
%% Spezielle Einstellungen, insbesondere fuer das Literaturverzeichnis,
% aber auch Packages wie amsmath, Groessenanpassungen etc.
\input{Preferences.tex}
%

% Hier werden die Referenzen in einer separaten Datei gespeichert
\addbibresource{Thesis.bib}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------
%  Informationen
%----------------------------------------------------------------------------------
\author{Moritz Palm}
\title{Comparative Analysis of Predictive Performance: Neural Networks vs. GRU in League of
Legends Match Outcome Prediction}
\date{\today}


\input{abbreviations} % Abkuerzungen
 
%----------------------------------------------------------------------------------
%  Anfang des Dokuments
%----------------------------------------------------------------------------------
\begin{document}
\pagenumbering{Roman} % grosse Roemische Seitenummerierung
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ********************** Titelseite *********************** %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter
\begin{titlepage}
\begin{figure}[thb]
       \includegraphics[height=2.3cm]{./images/logo/FakIM_Logo} 
\end{figure}
\begin{center}
\rule{0pt}{0pt}
\vfill
\vfill
\vfill
\vfill

\begin{huge}
\@title\\[0.75ex]
\end{huge}

\vfill
\vfill


Bachelorarbeit\\ von\\

\vspace*{.5cm}
\textbf{\@author}\\
Matrikelnummer: 1234567
\vspace{.5cm}

\vfill
\vfill
\textbf{\large Fakultät Informatik und Mathematik\\
Ostbayerische Technische Hochschule Regensburg\\
(OTH Regensburg)}
\vfill
\vfill

\begin{tabular}{rl}
Gutachter:   		& Prof. Dr. Brijnesh Jain\\
Zweitgutachter:   	& Prof. Dr. Timo Baumann\\
%Betreuer:   		& Dr. Max Mustermann\\
\\Abgabedatum:& \@date
\end{tabular}
\end{center}
\end{titlepage}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ****************** Erklärung zur Arbeit ***************** %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\text{~}
\vspace{11cm}

\noindent
Herr\\
\@author\\
Konrad-Adenauer-Allee 55\\
93051 Regensburg\\
\smallskip

\noindent
Studiengang: Künstliche Intelligenz \& Data Science
\bigskip

\begin{enumerate}
\item Mir ist bekannt, dass dieses Exemplar der Bachelorarbeit als Prüfungsleistung in das Eigentum des Freistaates Bayern übergeht.
\item Ich erkläre hiermit, dass ich diese Bachelorarbeit selbstständig verfasst, noch nicht anderweitig für Prüfungszwecke vorgelegt, keine anderen als die angegebenen Quellen und Hilfsmittel benutzt sowie wörtlich und sinngemäße Zitate als solche gekennzeichnet habe.
\end{enumerate}
\vspace{1cm}
Regensburg, den \@date\\
\medskip
\medskip

\noindent
\underline{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\
\@author

\makeatother




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ******************* Inhaltsverzeichnis ****************** %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage
\pdfbookmark{\contentsname}{toc}\tableofcontents 										% Inhaltsverzeichnis




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ******************* Beginn des Textes ******************* %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{scrheadings} 																% normale Kopf- und Fusszeilen fuer den Rest
\cleardoublepage
\pagenumbering{arabic} 																	% ab jetzt arabische Nummerierung


\chapter{Introduction}
\label{chap:intro}


esports is highly relevant due to it being a huge and strongly growing market.
Esports and mobas in particular are hard to understand and follow. A live game prediction view can help fans understand the action and decisions made better and help immerse the audience by detecting upsets and swings in win probability.
many games are hard to understand, due to lots of information being displayed with very little explanation
a win predicition graph can help viewers understand the action and the significance of certain plays better, thus increasing engagement and enjoyment.
riot games has already implemented their own proprietary win prediction
a win prediction model can also help players make more informed decisions about what the optimal path of actions is

the model should be able to answer the question, if team a is far enough ahead to win or if team b with their hyperscaling heroes can come back and win

\chapter{Background}
\label{chap:background}

\section{League of Legends}
\label{sec:LoL}

League of Legends is a \ac{moba} game developed by Riot Games.
MOBA games are a subgenre of real-time strategy games in which two teams, typically consisting of five players ('summoners') each, compete against each other with each player controlling a single character \cite{mora-cantallopsMOBAGamesLiterature2018}, called 'champion'.
It is one of the most played video game genres CITATION NEEDED and attracts millions of players and fans watching the professional scene.
Most \acp{moba} differ only slightly in terms of basic gameplay or map layout, but vary in details such as champions, abilities, graphics etc.
As League of Legends is the most played game in the \ac{moba} genre, we will focus on it.
At the start of the game, each player picks a champion from a pool of currently 165 champions, each with distinct abilities and characteristics.
The map consists of two bases which are connected by three lanes.
Each base contains a large structure, the so called 'nexus', which is protected by two turrets. 
The goal of the game is to destroy the enemy nexus.
The bases are connected by three lanes, separated by a jungle.
\Acp{npc}, so called 'minions', spawn in regular intervals and advance down the lanes.
Killing minions grants gold and \ac{xp}, which are used to improve different attributes by buying items or increasing ones level.
It is conventional that the players split up at the start of the game, with one player going to the top lane, one to the mid lane and two players to the bottom lane.
The last player gets his gold and \ac{xp} from killing neutral monsters in the area between each lane, commonly referred to as 'the jungle'.
It also contains two large neutral monsters, Baron Nashor and a dragon, which require multiple team member to be killed and grant improvements to the whole team.
Larger fights are usually centered around either destroying a turret or killing a large neutral monster.
Every year, the game enters a new 'season', where it undergoes major challenges. 
In between seasons, the developers release smaller patches every two weeks, which are usually aimed at changing the strength of different champions.
These patches have the potential to cause major disruption to the best way the game is played (the so called 'meta'),  for example by introducing a new champion.

The players need to pick a champion that fits best into the team, taking into account the team strategy, the damage composition, the role the player has been assigned etc., while also selecting a champion the player has played before and can play well.
This is a very critical, yet highly difficult choice, as in theory there are $\binom{165}{10} = \frac{165!}{10! \cdot (165-10)!}$ different combinations of champions.




a modified elo system  is used to determine the skill of an individual player, which is the most important factor in which players get matched against each other


\section{Neural Networks}
\label{sec:nn}

\Acp{ann} have been originally designed to simulate the way the human brain processes information.
The fundamental unit in an \ac{ann} is called a neuron, first proposed by McCulloch and Pitts  \cite{mccullochLogicalCalculusIdeas1943}.
A neuron is a function which takes a number of values $ \mathbf{x} = (x_1, x_2, ... , x_m), m \in \mathbb{N}$ and outputs its activation $a$.
The weighted input $z$ is calculated as the dot product of the input vector $\mathbf{x}$ and the weight vector $\mathbf{w} = (w_1, w_2, ... , w_m)$ with the bias $b$ added:
\begin{equation}
z = w_1 x_1 + ... + w_m x_m + b = \mathbf{w}^T \mathbf{x} + b.
\end{equation}
$z$ is then passed through an activation function $\phi$, which provides non-linearity:
$a = \phi(z) = \phi(\mathbf{w}^T \mathbf{x})$.
Common activation functions include sigmoid functions, \ac{relu} and more.
The \ac{mlp}, first introduced by Rosenblatt \cite{rosenblattPerceptronProbabilisticModel1958a} organizes neurons in layers.
Input data is fed into the input layer, and computations propagate forward through the hidden layers to produce an output.
In the simplest form of an ANN, every neuron in a layer is connected to every neuron in the subsequent layer, forming a fully connected architecture.
There are no other connections, making it a feed-forward network, and it constitutes a directed acyclic graph.
A fully connected feed-forward network network can be described as a series of matrix-vector multiplications for each layer $l$:
\begin{equation}
\label{eq:nn}
\mathbf{a}^{(l)} = \phi (\mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)})
\end{equation}
In equation \ref{eq:nn}, $\mathbf{a}^{(l)}$ denotes the activation of layer $l$, $\mathbf{W}^{(l)}$ denotes the weight matrix of layer $l$ and $\mathbf{b}^{(l)}$ is the corresponding bias vector with $d$ being the number of neurons per layer.
If $l$ is the output layer $L$, the vector $\mathbf{a}^{(L)}$ represents the predicted output $\hat{y}$ of the network.

To allow the network to approximate any measurable function, at least one hidden layer is required \cite{hornikMultilayerFeedforwardNetworks1989}.
During the training process, the weights and biases are iteratively adjusted using the backpropagation algorithm to minimize the loss function $E$.
For regression tasks, a commonly used loss function is the \ac{mse} Loss
\begin{equation}
E_N = \frac{1}{N} \sum_{i=1}^{N}(y_{i} - \hat{y}_{i})^2
\end{equation}
where $N$ is the number of samples \cite{lianoRobustErrorMeasure1996a}.
 For binary classification, the \ac{cel}
\begin{equation}
E_N = \frac{1}{N} \sum_{k=1}^{N} y_k \ln{\hat{y}_k} + (1-y_k) \ln{(1-\hat{y}_k)}
\end{equation}
is widely used \cite{zhangGeneralizedCrossEntropy2018}.





\section{Recurrent Neural Networks}
\label{sec:rnn}
\Acp{rnn} build upon the foundation of feed-forward neural networks. They are designed to handle sequential data by introducing the concept of recurrence. In an \ac{rnn}, the output at each time step depends not only on the current input but also on the network's previous internal state or hidden state. This allows \acp{rnn} to capture dependencies over time, making them suitable for tasks involving sequential input such as speech and language \cite{lecunDeepLearning2015}.
The first fully connected \ac{rnn} was proposed by Elman \cite{elmanFindingStructureTime1990}.
\Acp{rnn} maintain a 'state vector' in their hidden units (see figure \ref{fig:elman}), implicitly containing information extracted from all the past elements of the sequence \cite{lecunDeepLearning2015}.
The output $\hat{y}$ of a neural net can be calculated by
\begin{equation}
\label{eq:rnn_output}
\hat{y}_t = \tanh(V h_t + b_o)
\end{equation}
where $V$ and $b_o$ are the weight matrix and the bias vector of the cell output.
In addition to calculating the output, the forward propagation involves updating the hidden state $h_t$ at every timestep $t$:
\begin{equation}
\label{eq:rnn_update}
h_t =
\begin{cases}
	0, & \text{if}\ t = 0 \\
	\sigma (W x_t + U h_{t-1} +  b_h), & \text{otherwise}
\end{cases}
\end{equation}
where $\sigma$ is the sigmoid function, $U$ and $b_h$ are the weight matrix and the bias of the hidden state and $W$ is the weight matrix of the input.
For an \ac{rnn} with just one recurrent layer, one hidden weight matrix $W_h$ is sufficient, as the weights are shared across timesteps.

\Citeauthor{bengioLearningLongtermDependencies1994a} \cite{bengioLearningLongtermDependencies1994a} showed, however, that \acp{rnn} have challenges with exploding or vanishing gradients, especially in long sequences, which can affect their ability to capture long-range dependencies \cite{sutskeverTrainingRecurrentNeural2013}.

\begin{figure}
\centering
\begin{tikzpicture}[item/.style={circle,draw,thick,align=center},
itemc/.style={item,on chain,join}]
 \begin{scope}[start chain=going right,nodes=itemc,every
 join/.style={-latex,very thick},local bounding box=chain]
 \path node (A0) {$h_{t-1}$} node (A1) {$h_t$} node (A2) {$A$} node[xshift=2em] (At)
 {$A$};
 \end{scope}
 \node[left=1em of chain,scale=2] (eq) {$=$};
 \node[left=2em of eq,item] (AL) {$h$};
 \path (AL.west) ++ (-1em,2em) coordinate (aux);
 \draw[very thick,-latex,rounded corners] (AL.east) -| ++ (1em,2em) -- (aux)
 |- (AL.west);
 \foreach \X in {0,1,2,t}
 {\draw[very thick,-latex] (A\X.north) -- ++ (0,2em)
 node[above,item,fill=gray!10] (h\X) {$y_\X$};
 \draw[very thick,latex-] (A\X.south) -- ++ (0,-2em)
 node[below,item,fill=gray!10] (x\X) {$y_\X$};}
 \draw[white,line width=0.8ex] (AL.north) -- ++ (0,1.9em);
 \draw[very thick,-latex] (AL.north) -- ++ (0,2em)
 node[above,item,fill=gray!10] {$y_t$};
 \draw[very thick,latex-] (AL.south) -- ++ (0,-2em)
 node[below,item,fill=gray!10] {$x_t$};
 \path (x2) -- (xt) node[midway,scale=2,font=\bfseries] {\dots};
\end{tikzpicture}
\caption{Unrolling of a \ac{rnn} over time}
\label{fig:rnn_unroll}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
\draw[rounded corners] (0, 0) rectangle (8, 4) {};
\draw[rounded corners, dashed] (1.5,1.5) rectangle (2.5,2.5) node[pos=.5] {$h_t$};
\draw[rounded corners, dashed] (5.5,1.5) rectangle (6.5,2.5) node[pos=.5] {$Z_t$};
\end{tikzpicture}
\caption{Elman Recurrent Unit}
\label{fig:elman}
\end{figure}


\begin{figure}
\centering
\begin{tikzpicture}
\node[block, draw=black, minimum width=8cm, minimum height=4cm](border) {};
%\coordinate (center east) at (border.shape center -| border.east);
\node[block, draw=black, minimum width=1cm, minimum height=1cm,align=left] (1) {$h_t$};
\node[block, draw=black, minimum width=1cm, minimum height=1cm, right=of 1] (2) {$Z_t$};
\end{tikzpicture}
\caption{Gated Recurrent Unit}
\label{fig:gru}
\end{figure}

\section{GRU}
\label{sec:gru}
In order to overcome the exploding/vanishing gradient problem of vanilla \acp{rnn}, gated networks like the \ac{lstm} \cite{hochreiterLongShortTermMemory1997} and \ac{gru} \cite{choLearningPhraseRepresentations2014} have been developed \cite{vanhoudtReviewLongShortterm2020}.
As they introduce an increased number of parameters compared to traditional \acp{rnn}, gated networks like the \ac{lstm} and \ac{gru} demand greater computational power \cite{deyGatevariantsGatedRecurrent2017}.
Compared to the \ac{lstm} network, \ac{gru} reduces the number of gate networks to two, thus being simpler to implement and compute \cite{choLearningPhraseRepresentations2014}.
\Citeauthor{chungEmpiricalEvaluationGated2014} even found that \ac{gru} is at least comparable to \ac{lstm} most of the time \cite{chungEmpiricalEvaluationGated2014}.
The gates control the activation of each hidden unit.
The reset gate is calculated by
\begin{equation}
\label{eq:gru_reset}
r_t = \sigma (W_r x_t + U_r h_{t-1} + b_r)
\end{equation}
and the update gate $z_j$ by
\begin{equation}
\label{eq:gru_update}
z_t = \sigma (W_z x_t + U_z h_{t-1} + b_z)
\end{equation}
\cite{deyGatevariantsGatedRecurrent2017}.
The hidden state update is a linear interpolation between the previous activation $h_{t-1}$ and the candidate activation $\tilde{h}_t$, where the update gate $z_t$ influences how much the hidden state is changed \cite{chungEmpiricalEvaluationGated2014}:
\begin{equation}
\label{eq:gru_h}
h_t = (1-z_t) \odot h_t + z_t \odot \tilde{h}_t
\end{equation}
with
\begin{equation}
\label{eq_gru_h_tilde}
\tilde{h}_t = g(W_h x_t + U_h (r_t \odot h_{t-1} + b_h)
\end{equation}
.
In equations \ref{eq:gru_h} and \ref{eq_gru_h_tilde} $\odot$ denotes the elementwise (Hadamard) multiplication and the function $g$ in equation \ref{eq_gru_h_tilde} is the activation function.



\section{Related work}
\label{sec:related}

Utilizing machine learning methods to extract information from data generated by e-sport games is an area of ongoing research.
A lot of scientific research focuses on the similar \ac{moba} DotA 2, which has easier and more fine-grained data collection methods (see section \ref{sec:datacoll}).
Due to the high similarity between these two games, it is to be expected that any findings for one game can be replicated and used for the other game with minimal adaptations.
Nevertheless, to ensure a fair comparison, both games are presented separately below.

In DotA 2, a wide variety of algorithms have been used.  
\Citeauthor{yuMOBASliceTimeSlice2018} \cite{yuMOBASliceTimeSlice2018} trained a \ac{rnn} on 71,355 matches and achieved an accuracy of $0.7083$ at the half-way point of a match, which according to their analysis is on average at 20 minutes.
\Citeauthor{wangPredictingMultiplayerOnline2016} \cite{wangPredictingMultiplayerOnline2016} compared Logistic Regression with a \ac{fnn} trained on up to 911,468 matches, with \ac{lr} achieving a slightly better accuracy ($0.6104$) than the \ac{fnn} ($0.588$).


\Citeauthor{silvaContinuousOutcomePrediction2018} have used \acp{rnn} to predicting the winner using data of different time intervals. They achieved an accuracy of 75\% when using data from between the 10 and 15 minute mark.
An evaluation of LSTM resulted in lower accuracy, most likely due to the large amount of data required \cite{silvaContinuousOutcomePrediction2018}.






\chapter{Experiments}
\label{chap:experiments}

As two very different experiments are compared against each other, two different datasets need to be constructed: one dataset containing all relevant information prior to the start of the game and one dataset containing only the temporal information from the beginning of the game.


\section{Data Collection}
\label{sec:datacoll}

\paragraph{High-Rank Matches}
The focus of data acquisition was directed towards high-rank matches, in which a mix of excellent amateur and professional players play.
Lower rank matches are not considered due to their higher unpredictability as less skilled players make huge, game-changing mistakes way more often.
Pro matches, defined as professional players playing with their respective teams in an esport tournament or league, were not included as there are way less matches and they are not available through the official Riot Games API.
High rank matches in this context are defined as having at least one player holding the rank of Master, Grandmaster or Challenger.
Riot Games themselves considers any rank above Diamond 3 as 'Elite' \cite{riotgamesDevBalanceFramework2020}, but we raise this bar just slightly to only include any rank above Diamond 1.
Due to the fact that for a match to be included in the dataset, only one out of ten players needs to hold one of the aforementioned highest ranks, some slightly lower ranked players are also present in the dataset.
These ranks combined account for the top $0.2\%$ of all players \cite{riotgamesRankedTiersDivisions2023}.
\paragraph{Riot Games API}
The primary source of data stemmed from the Riot Games API, a comprehensive repository of information pertaining to League of Legends gameplay.
The Riot Games API provided access to a plethora of essential data points, including champion statistics, general match information, timeline details, and player-specific information.
These variables collectively form a comprehensive and multifaceted dataset crucial for the development of an effective predictive model.
\paragraph{Other Data Sources}
However, not all pertinent data were available directly from the Riot Games API.
These include the winning chance of each champion and statistics on how each player performs on each relevant champion.
To address this limitation, a web-scraping approach was employed to gather additional relevant information. The amalgamation of Riot Games API data and web-scraped data was meticulously organized and stored in a Database.
This central repository served as the foundational storehouse from which distinct datasets were constructed, ensuring an organized and structured approach to data management.
\paragraph{Regions}
Multiple regions were included in the data collection process, including Europe West (EUW), Europe Nordic \& East (EUN), Korea (KR), and North America (NA).
This regional diversity contributes to the model's generalizability across different player bases and playing styles.
\paragraph{Period of Time}
All matches included in the dataset were played in season 13 and on patch 20.
It is important that all matches are played on the same patch, as a patch may cause major shifts in the balance of the game, thus making certain strategies and champions way better than others.
\\ \  \\
In summary, the data collection process for this study involved a dual-pronged approach, leveraging the extensive resources provided by the Riot Games API alongside a targeted web-scraping strategy.
The resulting raw dataset containing $20,513$ and  $3,972$ matches in the pre-game and in-game datasets respectively, stored in a PostgreSQL Database, reflects a comprehensive compilation of high-rank amateur League of Legends matches.


\section{Data Processing}
\label{sec:data_processing}

\subsection{Game Dataset}
The raw pre-game dataset contains $368$ columns which can be categorized into three distinct groups: General Match Information, Player Information, Champion Information and Player-Champion Information.
General Information about the match (i.e. patch number) is only used for validation purposes and not included in the final dataset.
\paragraph{Player Information}
Player Information feature $x_p$ is a two-dimensional vector containing information about the player: the account level as an indication of how many games the player has played and the rank of the player as a measure of how good the player is
\paragraph{Champion Information}
The Champion Information feature $\mathbf{x}_c$ is composed of 

\paragraph{Player-Champion Information}
This feature vector contains information about the player on this particular champion.
This includes the average amount of gold earned by the player over all his matches played on this champion in season 13.




\chapter{Results}
\label{chap:results}


\chapter{Discussion}
\label{chap:discussion}


\chapter{Conclusion}
\label{chap:conclusion}

%\bibliographystyle{natdin}
%\bibliographystyle{naturemag}
%\bibliographystyle{geralpha}
\printbibliography


% Anhang
\include{appendix}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ********************* Verzeichnisse ********************* %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listoffigures																			% Abbildungsverzeichnis
\listoftables																			% Tabellenverzeichnis
\cleardoublepage\phantomsection\addcontentsline{toc}{chapter}{List of Abbreviations}	% Abkürzungsverzeichnis
\printacronyms[heading={chapter*}, name={List of Abbreviations}]



\end{document}
