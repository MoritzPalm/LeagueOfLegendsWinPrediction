{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.250852200Z",
     "start_time": "2023-10-30T14:37:27.046458400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor, optim, nn\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"architecture\": \"GRU\",\n",
    "    \"dataset\": \"timeline_1.0\",\n",
    "    \"epochs\": 50,\n",
    "    \"classes\": 2,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_layers\": 2,\n",
    "    \"hidden_size\": 64,\n",
    "    \"dropout_prob\": 0,\n",
    "    \"input_size\": 381,\n",
    "    \"output_size\": 2,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss\": \"CrossEntropyLoss\",\n",
    "    \"activation\": \"ReLU\",\n",
    "    \"initializer\": \"Xavier\",\n",
    "    \"regularization\": \"L2\",\n",
    "    \"regularization_lambda\": 0.01,\n",
    "    \"gru_layers\": 1,\n",
    "    \"sequence_length\": 16,\n",
    "\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.250852200Z",
     "start_time": "2023-10-30T14:37:27.055059200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    with wandb.init(project=\"leaguify\", config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, val_loader, test_loader, criterion, optimizer = make(config)\n",
    "        print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        train(model, train_loader, criterion, optimizer, config)\n",
    "\n",
    "        # and test its final performance\n",
    "        test(model, val_loader)\n",
    "        test(model, test_loader)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.250852200Z",
     "start_time": "2023-10-30T14:37:27.059068600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "class TimelineDataset(Dataset):\n",
    "    def __init__(self, data_dir, sequence_length, transform=None, target_transform=None):\n",
    "        self.data = torch.tensor(np.load(data_dir)[:, :-1], dtype=torch.float32, device=device)\n",
    "        self.labels = torch.tensor(np.load(data_dir)[:, -1], dtype=torch.int64, device=device)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx:idx + self.sequence_length, :]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return sample, label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.250852200Z",
     "start_time": "2023-10-30T14:37:27.064484200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    train, val = get_train_data(sequence_length=config.sequence_length, slice=2)\n",
    "    test = get_test_data(sequence_length=config.sequence_length)\n",
    "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
    "    val_loader = make_loader(val, batch_size=config.batch_size)\n",
    "    test_loader = make_loader(test, batch_size=config.batch_size)\n",
    "\n",
    "    model = GRU(config.input_size, config.hidden_size, config.classes, config.num_layers, config\n",
    "                .gru_layers, drop_prob=config.dropout_prob).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    return model, train_loader, val_loader, test_loader, criterion, optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.316503400Z",
     "start_time": "2023-10-30T14:37:27.070452100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "def get_train_data(sequence_length=16, val_split=0.8, slice=1) -> (torch.utils.data.Dataset, torch.utils.data.Dataset):\n",
    "    dataset = TimelineDataset('../data/processed/train_timeline.npy', sequence_length)\n",
    "    print(f'len(dataset): {len(dataset)}')\n",
    "    train_len = int(len(dataset) * val_split // sequence_length * sequence_length)\n",
    "    print(f'train_len: {train_len}')\n",
    "    assert train_len % sequence_length == 0\n",
    "    val_len = len(dataset) - train_len\n",
    "    assert val_len % sequence_length == 0\n",
    "    print(f'train_len: {train_len}, val_len: {val_len}')\n",
    "    train_data, val_data = torch.utils.data.random_split(dataset, [train_len, val_len])\n",
    "    train_slice = torch.utils.data.Subset(\n",
    "        train_data, indices=range(0, len(train_data), slice))\n",
    "    check_correct_split(dataset, train_data, val_data, sequence_length=sequence_length)\n",
    "    return train_slice, val_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.316503400Z",
     "start_time": "2023-10-30T14:37:27.074922500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "def check_correct_split(full_dataset, train_data, val_data, sequence_length=16):\n",
    "    \"\"\"\n",
    "    Check that the train/val split is correct and no data is leaked between the two\n",
    "    appends both datasets together, removes duplicates and checks that the original dataset is the same as the combined\n",
    "    :param full_dataset: \n",
    "    :param train_data: \n",
    "    :param val_data: \n",
    "    :param sequence_length: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    len_combined = len(full_dataset)\n",
    "    full_no_dup = torch.unique(full_dataset, dim=0)\n",
    "    len_no_dup = len(full_no_dup)\n",
    "    original = TimelineDataset('../data/processed/train_timeline.npy', sequence_length)\n",
    "    len_original = len(original)\n",
    "    print(f'len_combined: {len_combined}, len_no_dup: {len_no_dup}, len_original: {len_original}')\n",
    "    assert len_combined == len_no_dup\n",
    "    assert len_original == len_combined\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.316503400Z",
     "start_time": "2023-10-30T14:37:27.079957300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "def get_test_data(sequence_length=16):\n",
    "    full_dataset = TimelineDataset('../data/processed/test_timeline.npy', sequence_length)\n",
    "    return full_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.316503400Z",
     "start_time": "2023-10-30T14:37:27.084116700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "def make_loader(dataset, batch_size=64):\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=0, drop_last=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.316503400Z",
     "start_time": "2023-10-30T14:37:27.088665800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0+cu121\n",
      "**********\n",
      "_CUDA version: \n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Aug_15_22:09:35_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.140\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\n",
      "**********\n",
      "CUDNN version: 8801\n",
      "Available GPU devices: 1\n",
      "Device Name: NVIDIA GeForce RTX 2080\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    print(f'PyTorch version: {torch.__version__}')\n",
    "    print('*' * 10)\n",
    "    print(f'_CUDA version: ')\n",
    "    !nvcc --version\n",
    "    print('*' * 10)\n",
    "    print(f'CUDNN version: {torch.backends.cudnn.version()}')\n",
    "    print(f'Available GPU devices: {torch.cuda.device_count()}')\n",
    "    print(f'Device Name: {torch.cuda.get_device_name()}')\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.317501300Z",
     "start_time": "2023-10-30T14:37:27.094093100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, gru_layers, drop_prob=0.2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, gru_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:, -1]))\n",
    "        return out, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(1, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.317501300Z",
     "start_time": "2023-10-30T14:37:27.160365Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, config):\n",
    "    wandb.watch(model, criterion, log='all', log_freq=10)\n",
    "\n",
    "    total_batches = len(loader) * config.epochs\n",
    "    example_count = 0\n",
    "    batch_count = 0\n",
    "    loss_vals = []\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        h = model.init_hidden(config.batch_size)\n",
    "        for _, (matches, labels) in enumerate(loader):\n",
    "            output, h = model(matches)  # hidden state is not passed to re-init at each batch\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            example_count += len(matches)\n",
    "            batch_count += 1\n",
    "            if (batch_count + 1) % 25 == 0:\n",
    "                train_log(loss, example_count, epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.317501300Z",
     "start_time": "2023-10-30T14:37:27.164707Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "def train_log(loss, example_count, epoch):\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_count)\n",
    "    print(f\"Loss after {str(example_count).zfill(5)} examples: {loss:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.317501300Z",
     "start_time": "2023-10-30T14:37:27.170370800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset): 12784\n",
      "train_len: 10224\n",
      "train_len: 10224, val_len: 2560\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConcatDataset' object has no attribute 'tensor'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[157], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_data, val_data \u001B[38;5;241m=\u001B[39m \u001B[43mget_train_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequence_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_data: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(train_data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_data: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(val_data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[149], line 13\u001B[0m, in \u001B[0;36mget_train_data\u001B[1;34m(sequence_length, val_split, slice)\u001B[0m\n\u001B[0;32m     10\u001B[0m train_data, val_data \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mrandom_split(dataset, [train_len, val_len])\n\u001B[0;32m     11\u001B[0m train_slice \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mSubset(\n\u001B[0;32m     12\u001B[0m     train_data, indices\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(train_data), \u001B[38;5;28mslice\u001B[39m))\n\u001B[1;32m---> 13\u001B[0m \u001B[43mcheck_correct_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msequence_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msequence_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m train_slice, val_data\n",
      "Cell \u001B[1;32mIn[150], line 10\u001B[0m, in \u001B[0;36mcheck_correct_split\u001B[1;34m(train_data, val_data, sequence_length)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_correct_split\u001B[39m(train_data, val_data, sequence_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m):\n\u001B[0;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m    Check that the train/val split is correct and no data is leaked between the two\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m    appends both datasets together, removes duplicates and checks that the original dataset is the same as the combined\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124;03m    :return: \u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m     full_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mConcatDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_data\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     11\u001B[0m     len_combined \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(full_dataset)\n\u001B[0;32m     12\u001B[0m     full_no_dup \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39munique(full_dataset, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'ConcatDataset' object has no attribute 'tensor'"
     ]
    }
   ],
   "source": [
    "train_data, val_data = get_train_data(sequence_length=16)\n",
    "print(f'train_data: {len(train_data)}')\n",
    "print(f'val_data: {len(val_data)}')\n",
    "for matches, labels in make_loader(val_data, batch_size=1):\n",
    "    if matches.shape[1] != 16:\n",
    "        print(f'matches: {matches.shape}, labels: {labels.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:37:27.320502Z",
     "start_time": "2023-10-30T14:37:27.177385500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for matches, labels in test_loader:\n",
    "            print(f'matches: {matches.shape}, labels: {labels.shape}')\n",
    "            matches, labels = matches.to(device), labels.to(device)\n",
    "            model.eval()\n",
    "            output, h = model(matches)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy of the model on the {total} \" +\n",
    "              f\"test matches: {correct / total:%}\")\n",
    "\n",
    "        wandb.log({\"test_accuracy\": correct / total})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-30T14:37:27.282501200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model_pipeline(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-30T14:37:27.283502100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-30T14:37:27.284504300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
