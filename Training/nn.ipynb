{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.137461100Z",
     "start_time": "2023-11-16T15:36:50.851479400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from torch import optim, nn\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"architecture\": \"NN\",\n",
    "    \"dataset\": \"static_1.1\",\n",
    "    \"epochs\": 50,\n",
    "    \"classes\": 2,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_layers\": 15,\n",
    "    \"hidden_size\": 16384,\n",
    "    \"dropout_prob\": 0.1,\n",
    "    \"input_size\": 329,\n",
    "    \"output_size\": 1,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss\": \"CrossEntropyLoss\",\n",
    "    \"activation\": \"ReLU\",\n",
    "    \"decrease_size\": True,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.138460Z",
     "start_time": "2023-11-16T15:36:50.857014900Z"
    }
   },
   "id": "e13812e0d04c772b"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    with wandb.init(project=\"leaguify\", config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, val_loader, test_loader, criterion, optimizer = make(config)\n",
    "        print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        train(model, train_loader, val_loader, criterion, optimizer, config)\n",
    "\n",
    "        # and test its final performance\n",
    "        test(model, val_loader)\n",
    "        test(model, test_loader)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.138460Z",
     "start_time": "2023-11-16T15:36:50.860769500Z"
    }
   },
   "id": "bbb0d7d96615b112"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def get_activation(name):\n",
    "    if name == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    elif name == 'LeakyReLU':\n",
    "        return nn.LeakyReLU()\n",
    "    elif name == 'ELU':\n",
    "        return nn.ELU()\n",
    "    elif name == 'SELU':\n",
    "        return nn.SELU()\n",
    "    elif name == 'Tanh':\n",
    "        return nn.Tanh()\n",
    "    elif name == 'Sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    else:\n",
    "        raise ValueError(f'Activation {name} not supported')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.138460Z",
     "start_time": "2023-11-16T15:36:50.865786300Z"
    }
   },
   "id": "2c0e42859ee4428f"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "class StaticDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, target_transform=None):\n",
    "        self.data = torch.tensor(np.load(data_dir)[:, :-1], dtype=torch.float32, device=device)\n",
    "        self.labels = torch.tensor(np.load(data_dir)[:, -1], dtype=torch.int64, device=device)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.print_statistics()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx, 1:]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return sample, label\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print(f'Number of samples: {len(self.data)}')\n",
    "        print(f'Number of features: {len(self.data[0])}')\n",
    "        print(f'Number of classes: {len(np.unique(self.labels.cpu().numpy()))}')\n",
    "        print(f'Number of samples per class: {np.bincount(self.labels.cpu().numpy())}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.165464Z",
     "start_time": "2023-11-16T15:36:50.872493600Z"
    }
   },
   "id": "c843eaf2d2fe8be7"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    train_data, val_data = get_train_data()\n",
    "    train_loader = make_loader(train_data, batch_size=config.batch_size)\n",
    "    val_loader = make_loader(val_data, batch_size=config.batch_size)\n",
    "    test_loader = make_loader(get_test_data(), batch_size=config.batch_size)\n",
    "    activation = get_activation(config.activation)\n",
    "\n",
    "    model = NeuralNetwork(config.input_size, config.hidden_size, config.num_layers, config.dropout_prob,\n",
    "                          config.classes, activation, config.decrease_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if config.optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config.learning_rate)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    return model, train_loader, val_loader, test_loader, criterion, optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.165464Z",
     "start_time": "2023-11-16T15:36:50.878469200Z"
    }
   },
   "id": "3ccb742896a9db03"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def get_train_data(val_split=0.8):\n",
    "    dataset = StaticDataset('../data/processed/train_static.npy')\n",
    "    train_len = int(len(dataset) * val_split)\n",
    "    val_len = len(dataset) - train_len\n",
    "    print(f'train_len: {train_len}, val_len: {val_len}')\n",
    "    return torch.utils.data.random_split(dataset, [train_len, val_len])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.165464Z",
     "start_time": "2023-11-16T15:36:50.883067400Z"
    }
   },
   "id": "7a0d955b1c217921"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    return StaticDataset('../data/processed/test_static.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.165464Z",
     "start_time": "2023-11-16T15:36:50.885589500Z"
    }
   },
   "id": "4ff9a0b79c444569"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def make_loader(dataset, batch_size=64):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.165464Z",
     "start_time": "2023-11-16T15:36:50.890565600Z"
    }
   },
   "id": "7feb26d9fd1afd97"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0+cu121\n",
      "**********\n",
      "_CUDA version: \n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Aug_15_22:09:35_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.140\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\n",
      "**********\n",
      "CUDNN version: 8801\n",
      "Available GPU devices: 1\n",
      "Device Name: NVIDIA GeForce RTX 2080\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    print(f'PyTorch version: {torch.__version__}')\n",
    "    print('*' * 10)\n",
    "    print(f'_CUDA version: ')\n",
    "    !nvcc --version\n",
    "    print('*' * 10)\n",
    "    print(f'CUDNN version: {torch.backends.cudnn.version()}')\n",
    "    print(f'Available GPU devices: {torch.cuda.device_count()}')\n",
    "    print(f'Device Name: {torch.cuda.get_device_name()}')\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.166464Z",
     "start_time": "2023-11-16T15:36:50.895569200Z"
    }
   },
   "id": "aace44d3e59a63c3"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob, output_size=1, activation=nn.ReLU(),\n",
    "                 decrease_size=False):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential()\n",
    "        self.linear_relu_stack.append(nn.Linear(input_size, hidden_size))\n",
    "        for i in range(num_layers - 1):\n",
    "            if decrease_size:\n",
    "                next_hidden_size = int(self.hidden_size // 2)\n",
    "            else:\n",
    "                next_hidden_size = self.hidden_size\n",
    "            self.linear_relu_stack.append(self.dropout)\n",
    "            self.linear_relu_stack.append(nn.BatchNorm1d(self.hidden_size))\n",
    "            self.linear_relu_stack.append(nn.Linear(self.hidden_size, next_hidden_size))\n",
    "            self.linear_relu_stack.append(activation)\n",
    "            self.hidden_size = next_hidden_size\n",
    "        self.linear_relu_stack.append(nn.Linear(self.hidden_size, self.output_size))\n",
    "        self.linear_relu_stack.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.166464Z",
     "start_time": "2023-11-16T15:36:50.975109900Z"
    }
   },
   "id": "3a87d27c212fc895"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "    example_count = 0\n",
    "    batch_count = 0\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        model.train()\n",
    "        for _, (matches, labels) in enumerate(train_loader):\n",
    "            matches, labels = matches.to(device), labels.to(device)\n",
    "            loss = train_batch(matches, labels, model, optimizer, criterion)\n",
    "            example_count += len(matches)\n",
    "            batch_count += 1\n",
    "            if ((batch_count + 1) % 25) == 0:\n",
    "                train_log(loss, example_count, epoch)\n",
    "                test(model, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.166464Z",
     "start_time": "2023-11-16T15:36:50.979992300Z"
    }
   },
   "id": "8639047e162f0756"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def train_batch(matches, labels, model, optimizer, criterion):\n",
    "    matches, labels = matches.to(device), labels.to(device)\n",
    "\n",
    "    # Forward pass ➡\n",
    "    outputs = model(matches)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.166464Z",
     "start_time": "2023-11-16T15:36:50.985396800Z"
    }
   },
   "id": "b21b9868aaeaebac"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "def train_log(loss, example_count, epoch):\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_count)\n",
    "    print(f\"Loss after {str(example_count).zfill(5)} examples: {loss:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.166464Z",
     "start_time": "2023-11-16T15:36:50.989919300Z"
    }
   },
   "id": "1d4afe9b0efd101a"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for matches, labels in test_loader:\n",
    "            matches, labels = matches.to(device), labels.to(device)\n",
    "            outputs = model(matches)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(classification_report(y_pred, y_true))\n",
    "\n",
    "        print(f\"Accuracy of the model on the {total} \" +\n",
    "              f\"test matches: {correct / total:%}\")\n",
    "\n",
    "        wandb.log({\"test_accuracy\": correct / total})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:36:51.166464Z",
     "start_time": "2023-11-16T15:36:50.995701700Z"
    }
   },
   "id": "7e2790071f2ec71e"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\morit\\Documents\\Informatik\\Projekte\\LeagueOfLegendsWinPrediction\\Training\\wandb\\run-20231116_163651-yuej487i</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/moritz-palm/leaguify/runs/yuej487i' target=\"_blank\">prime-bird-9484</a></strong> to <a href='https://wandb.ai/moritz-palm/leaguify' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/moritz-palm/leaguify' target=\"_blank\">https://wandb.ai/moritz-palm/leaguify</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/moritz-palm/leaguify/runs/yuej487i' target=\"_blank\">https://wandb.ai/moritz-palm/leaguify/runs/yuej487i</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 16159\n",
      "Number of features: 330\n",
      "Number of classes: 2\n",
      "Number of samples per class: [7692 8467]\n",
      "train_len: 12927, val_len: 3232\n",
      "Number of samples: 1995\n",
      "Number of features: 330\n",
      "Number of classes: 2\n",
      "Number of samples per class: [ 959 1036]\n",
      "NeuralNetwork(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=329, out_features=16384, bias=True)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "    (2): BatchNorm1d(16384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=16384, out_features=8192, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Linear(in_features=8192, out_features=4096, bias=True)\n",
      "    (8): ReLU()\n",
      "    (9): Dropout(p=0.1, inplace=False)\n",
      "    (10): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    (12): ReLU()\n",
      "    (13): Dropout(p=0.1, inplace=False)\n",
      "    (14): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (16): ReLU()\n",
      "    (17): Dropout(p=0.1, inplace=False)\n",
      "    (18): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (20): ReLU()\n",
      "    (21): Dropout(p=0.1, inplace=False)\n",
      "    (22): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (24): ReLU()\n",
      "    (25): Dropout(p=0.1, inplace=False)\n",
      "    (26): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (28): ReLU()\n",
      "    (29): Dropout(p=0.1, inplace=False)\n",
      "    (30): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (32): ReLU()\n",
      "    (33): Dropout(p=0.1, inplace=False)\n",
      "    (34): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (36): ReLU()\n",
      "    (37): Dropout(p=0.1, inplace=False)\n",
      "    (38): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (40): ReLU()\n",
      "    (41): Dropout(p=0.1, inplace=False)\n",
      "    (42): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (43): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (44): ReLU()\n",
      "    (45): Dropout(p=0.1, inplace=False)\n",
      "    (46): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (47): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (48): ReLU()\n",
      "    (49): Dropout(p=0.1, inplace=False)\n",
      "    (50): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (51): Linear(in_features=4, out_features=2, bias=True)\n",
      "    (52): ReLU()\n",
      "    (53): Dropout(p=0.1, inplace=False)\n",
      "    (54): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (55): Linear(in_features=2, out_features=1, bias=True)\n",
      "    (56): ReLU()\n",
      "    (57): Linear(in_features=1, out_features=2, bias=True)\n",
      "    (58): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01536 examples: 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\morit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\morit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.48      0.64      3232\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.48      3232\n",
      "   macro avg       0.50      0.24      0.32      3232\n",
      "weighted avg       1.00      0.48      0.64      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 47.586634%\n",
      "Loss after 03136 examples: 0.693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.49      0.52      1732\n",
      "           1       0.48      0.54      0.50      1500\n",
      "\n",
      "    accuracy                           0.51      3232\n",
      "   macro avg       0.51      0.51      0.51      3232\n",
      "weighted avg       0.51      0.51      0.51      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 50.990099%\n",
      "Loss after 04736 examples: 0.693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.48      0.64      3151\n",
      "           1       0.03      0.54      0.05        81\n",
      "\n",
      "    accuracy                           0.48      3232\n",
      "   macro avg       0.50      0.51      0.34      3232\n",
      "weighted avg       0.95      0.48      0.63      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 47.803218%\n",
      "Loss after 06336 examples: 0.693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.48      0.64      3178\n",
      "           1       0.02      0.54      0.03        54\n",
      "\n",
      "    accuracy                           0.48      3232\n",
      "   macro avg       0.50      0.51      0.34      3232\n",
      "weighted avg       0.97      0.48      0.63      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 47.710396%\n",
      "Loss after 07936 examples: 0.693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.48      0.64      3183\n",
      "           1       0.02      0.59      0.03        49\n",
      "\n",
      "    accuracy                           0.48      3232\n",
      "   macro avg       0.50      0.53      0.34      3232\n",
      "weighted avg       0.97      0.48      0.63      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 47.865099%\n",
      "Loss after 09536 examples: 0.693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.48      0.64      3186\n",
      "           1       0.02      0.61      0.03        46\n",
      "\n",
      "    accuracy                           0.48      3232\n",
      "   macro avg       0.50      0.54      0.34      3232\n",
      "weighted avg       0.97      0.48      0.63      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 47.896040%\n",
      "Loss after 11136 examples: 0.693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.48      0.64      3186\n",
      "           1       0.02      0.61      0.03        46\n",
      "\n",
      "    accuracy                           0.48      3232\n",
      "   macro avg       0.50      0.54      0.34      3232\n",
      "weighted avg       0.97      0.48      0.63      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 47.896040%\n",
      "Loss after 12736 examples: 0.693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.48      0.64      3186\n",
      "           1       0.02      0.61      0.03        46\n",
      "\n",
      "    accuracy                           0.48      3232\n",
      "   macro avg       0.50      0.54      0.34      3232\n",
      "weighted avg       0.97      0.48      0.63      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 47.896040%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [04:37<3:46:23, 277.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 14335 examples: 0.702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.46      0.40      1198\n",
      "           1       0.62      0.52      0.56      2034\n",
      "\n",
      "    accuracy                           0.50      3232\n",
      "   macro avg       0.49      0.49      0.48      3232\n",
      "weighted avg       0.52      0.50      0.50      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 49.504950%\n",
      "Loss after 15935 examples: 0.692\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.48      0.61      2727\n",
      "           1       0.16      0.52      0.24       505\n",
      "\n",
      "    accuracy                           0.48      3232\n",
      "   macro avg       0.50      0.50      0.42      3232\n",
      "weighted avg       0.74      0.48      0.55      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 48.298267%\n",
      "Loss after 17535 examples: 0.695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.48      0.64      3148\n",
      "           1       0.03      0.57      0.05        84\n",
      "\n",
      "    accuracy                           0.48      3232\n",
      "   macro avg       0.50      0.52      0.35      3232\n",
      "weighted avg       0.95      0.48      0.63      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 47.957921%\n",
      "Loss after 19135 examples: 0.693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.48      0.64      3227\n",
      "           1       0.00      0.60      0.00         5\n",
      "\n",
      "    accuracy                           0.48      3232\n",
      "   macro avg       0.50      0.54      0.32      3232\n",
      "weighted avg       1.00      0.48      0.64      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 47.617574%\n",
      "Loss after 20735 examples: 0.693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.48      0.64      3231\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.48      3232\n",
      "   macro avg       0.50      0.24      0.32      3232\n",
      "weighted avg       1.00      0.48      0.64      3232\n",
      "\n",
      "Accuracy of the model on the 3232 test matches: 47.555693%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [06:50<5:35:03, 410.27s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\Temp\\ipykernel_20652\\2392872062.py\", line 10, in model_pipeline\n",
      "    train(model, train_loader, val_loader, criterion, optimizer, config)\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\Temp\\ipykernel_20652\\1603276510.py\", line 9, in train\n",
      "    loss = train_batch(matches, labels, model, optimizer, criterion)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\Temp\\ipykernel_20652\\146055341.py\", line 5, in train_batch\n",
      "    outputs = model(matches)\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1568, in _call_impl\n",
      "    result = forward_call(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\Temp\\ipykernel_20652\\921294259.py\", line 28, in forward\n",
      "    logits = self.linear_relu_stack(x)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\morit\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁█████</td></tr><tr><td>loss</td><td>▅▂▂▂▂▂▂▂█▁▃▂▂</td></tr><tr><td>test_accuracy</td><td>▁█▂▁▂▂▂▂▅▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>0.69315</td></tr><tr><td>test_accuracy</td><td>0.47556</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">prime-bird-9484</strong> at: <a href='https://wandb.ai/moritz-palm/leaguify/runs/yuej487i' target=\"_blank\">https://wandb.ai/moritz-palm/leaguify/runs/yuej487i</a><br/> View job at <a href='https://wandb.ai/moritz-palm/leaguify/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTk5NjA5Nw==/version_details/v69' target=\"_blank\">https://wandb.ai/moritz-palm/leaguify/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTk5NjA5Nw==/version_details/v69</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20231116_163651-yuej487i\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[67], line 10\u001B[0m, in \u001B[0;36mmodel_pipeline\u001B[1;34m(hyperparameters)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(model)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# and use them to train the model\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# and test its final performance\u001B[39;00m\n\u001B[0;32m     13\u001B[0m test(model, val_loader)\n",
      "Cell \u001B[1;32mIn[76], line 9\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_loader, val_loader, criterion, optimizer, config)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, (matches, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m      8\u001B[0m     matches, labels \u001B[38;5;241m=\u001B[39m matches\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 9\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmatches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     example_count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(matches)\n\u001B[0;32m     11\u001B[0m     batch_count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[1;32mIn[77], line 5\u001B[0m, in \u001B[0;36mtrain_batch\u001B[1;34m(matches, labels, model, optimizer, criterion)\u001B[0m\n\u001B[0;32m      2\u001B[0m matches, labels \u001B[38;5;241m=\u001B[39m matches\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Forward pass ➡\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmatches\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Backward pass ⬅\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1568\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1565\u001B[0m     bw_hook \u001B[38;5;241m=\u001B[39m hooks\u001B[38;5;241m.\u001B[39mBackwardHook(\u001B[38;5;28mself\u001B[39m, full_backward_hooks, backward_pre_hooks)\n\u001B[0;32m   1566\u001B[0m     args \u001B[38;5;241m=\u001B[39m bw_hook\u001B[38;5;241m.\u001B[39msetup_input_hook(args)\n\u001B[1;32m-> 1568\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1569\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n\u001B[0;32m   1570\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m (\n\u001B[0;32m   1571\u001B[0m         \u001B[38;5;241m*\u001B[39m_global_forward_hooks\u001B[38;5;241m.\u001B[39mitems(),\n\u001B[0;32m   1572\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mitems(),\n\u001B[0;32m   1573\u001B[0m     ):\n\u001B[0;32m   1574\u001B[0m         \u001B[38;5;66;03m# mark that always called hook is run\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[75], line 28\u001B[0m, in \u001B[0;36mNeuralNetwork.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     27\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflatten(x)\n\u001B[1;32m---> 28\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear_relu_stack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logits\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    164\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    166\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mean\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_var\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbn_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexponential_average_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\leaguify-VaCbhr8h-py3.11\\Lib\\site-packages\\torch\\nn\\functional.py:2478\u001B[0m, in \u001B[0;36mbatch_norm\u001B[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[0;32m   2475\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[0;32m   2476\u001B[0m     _verify_batch_size(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[1;32m-> 2478\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2479\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_mean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_var\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\n\u001B[0;32m   2480\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = model_pipeline(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:43:55.080370300Z",
     "start_time": "2023-11-16T15:36:50.999025500Z"
    }
   },
   "id": "63223f3b223d001a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T15:43:55.081378200Z",
     "start_time": "2023-11-16T15:43:55.081378200Z"
    }
   },
   "id": "7797f9f08cbc4ded"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
